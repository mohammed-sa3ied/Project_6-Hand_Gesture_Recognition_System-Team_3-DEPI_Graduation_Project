# âœ‹ Hand Gesture Recognition System  
**Team 3 â€” Project 6**  
ğŸ“ Graduation Project â€” Digital Egypt Pioneers Initiative (DEPI) / Microsoft Machine Learning Engineer Program  

A deep-learning powered system that recognizes hand gestures from images and real-time video. Built using computer vision, convolutional neural networks (CNNs), and OpenCV, this project enables gesture-based interaction for applications such as HCI, VR, gaming, and accessibility tools.

---

## ğŸ“„ Table of Contents
* [Overview](#overview)
* [DEPI Graduation Project Statement](#depi-graduation-project-statement)
* [Features](#features)
* [Model Architecture](#model-architecture)
* [Project Structure](#project-structure)
* [Installation](#installation)
* [Dataset & Preprocessing](#dataset--preprocessing)
* [Training & Evaluation](#training--evaluation)
* [Real-Time Gesture Recognition](#real-time-gesture-recognition)
* [Results](#results)
* [Team](#team)
* [Acknowledgments](#acknowledgments)
* [License](#license)

---

## ğŸ‘€ Overview  
The Hand Gesture Recognition System detects and classifies hand gestures using a trained deep-learning model.  
This work includes:  
- Dataset preprocessing & augmentation  
- CNN model training & evaluation  
- Real-time webcam-based gesture detection  
- Deployment-ready modules â€” enabling further integration  

---

## ğŸ“ DEPI Graduation Project Statement  
This repository contains the graduation project of DEPI for the Microsoft Machine Learning Engineer Program. It represents our final applied project, showcasing practical machine learning skills in computer vision, deep learning, and real-time AI deployment.

---

## âœ¨ Features  
-  Preprocessing pipeline for gesture images (resizing, normalization, augmentation)  
-  Data augmentation to improve model generalization  
-  CNN-based gesture classification (custom CNN and support for transfer learning)  
-  Real-time hand gesture detection using webcam + OpenCV  
-  Optional integration with UI / deployment modules (for real-world use)  
-  Model evaluation (accuracy, precision, recall, F1-score, confusion matrix)  

---

## âœ‹ Model Architecture  
The system supports custom CNN models (tailored to the dataset and gestures)  

---

## ğŸ‘¨â€ğŸ«  Project Structure  

```text
/
â”œâ”€â”€ Final Model/               â† Saved trained model  
â”œâ”€â”€ Notebooks/                 â† Jupyter notebooks (preprocessing, training, evaluation, MLflow)  
â”œâ”€â”€ Preprocessed Data/         â† Cleaned & augmented datasets  
â”œâ”€â”€ Presentations/             â† Project slides & demo material  
â”œâ”€â”€ Reports/                   â† Final reports, documentation
â”œâ”€â”€ Streamlit UI App/          â† Web-app for real-time gesture recognition
â”œâ”€â”€ .gitignore                 â† The .gitignore file      
â”œâ”€â”€ README.md                  â† This file  
â”œâ”€â”€ LICENSE                    â† MIT License      
â””â”€â”€ requirements.txt           â† Required libraries and packages
```

---

## âš™ï¸ Installation

To run this project locally, follow these steps:

1.  **Clone the repository**
    ```bash
    git clone [https://github.com/ahmedbarakatt1/Project-6-Hand-Gesture-Recognition-System--Team-3.git](https://github.com/ahmedbarakatt1/Project-6-Hand-Gesture-Recognition-System--Team-3.git)
    cd Project-6-Hand-Gesture-Recognition-System--Team-3
    ```

2.  **Create and activate a virtual environment (Optional)**
    ```bash
    # Create the environment
    python -m venv venv

    # Activate the environment
    # Windows:
    venv\Scripts\activate
    # Mac/Linux:
    source venv/bin/activate
    ```

3.  **Install dependencies**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Launch the Streamlit UI**
    Run the following command to start the real-time gesture recognition app:
    ```bash
    cd "Streamlit UI App"
    streamlit run app.py
    ```
    
---

## ğŸ—‚ï¸ Dataset & Preprocessing

The dataset used in this project is the Kaggle hand gesture recognition dataset, composed by a set of near infrared images acquired by the Leap Motion sensor.
To ensure high-quality model training, the following preprocessing steps were applied:

- **Image resizing** for consistent input dimensions  
- **Normalization** to stabilize and speed up training  
- **Background removal** (optional) to reduce noise  
- **Data augmentation**, including:  
  - Rotation  
  - Flipping  
  - Scaling  
  - Brightness and contrast adjustments  

All processed and augmented data is stored inside the **Preprocessed Data/** directory to maintain reproducibility and allow consistent training across environments.

---

## ğŸ§  Training & Evaluation

Model training and evaluation are performed using Jupyter notebooks available in the **Notebooks/** directory.

This phase includes:

- Building and training **custom CNN** or **transfer-learning** models  
- Splitting the data into **training**, **validation**, and **testing** sets  
- Tracking key evaluation metrics:  
  - Accuracy  
  - Precision  
  - Recall  
  - F1-score  
  - Confusion matrix  
- Visualizing training progress using:  
  - Accuracy curves  
  - Loss curves  
- Saving trained models to the **Final Model/** directory for reusability and deployment  

---

## ğŸ¥ Real-Time Gesture Recognition

The real-time gesture recognition module enables live detection using a webcam feed.  

**Real-time processing includes:**

- **OpenCV** for webcam capture  
- **Preprocessing** applied to each incoming frame  
- **Model inference** to classify the gesture  
- **On-screen overlays** showing the predicted class  
- **Streamlit UI** for an interactive and user-friendly experience  

To run the live demo:

```bash
streamlit run "Streamlit UI App/app.py"
```

---

## ğŸ“Š Results

The project demonstrates strong performance in static hand-gesture recognition.  
Several evaluation visualizations are included in the notebooks, such as:

- **Training vs. validation accuracy curves**
- **Training vs. validation loss curves**
- **Confusion matrix** 

The real-time detection system performs reliably under typical lighting conditions and standard webcam setups, providing smooth and accurate gesture predictions during live testing.

---

## ğŸ‘¥ Team

*   Mohammed Saied â€” [LinkedIn](https://www.linkedin.com/in/mohammed-sa3ied?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app)
*   Aya Ashraf â€” [LinkedIn](https://www.linkedin.com/in/aya-ashraf15?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app)
*   Tasnim Qutb â€” [LinkedIn](https://www.linkedin.com/in/tasnim-qotb?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app)
*   Ahmed Barakat â€” [LinkedIn](https://www.linkedin.com/in/ahmad-barakat-4a16101b9?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app)
*   Mohammed Osama â€” [LinkedIn](https://www.linkedin.com/in/mohamed-abdulhalem?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app)

---

## ğŸ™ Acknowledgments
**Special thanks to:**
* DEPI & MCIT for mentorship and support
* Microsoft Machine Learning Engineer Program instructors
* Our colleagues and fellow trainees

---

## ğŸ“ License
This project is licensed under the **MIT License**.

You may use, modify, and distribute this project under its terms.
