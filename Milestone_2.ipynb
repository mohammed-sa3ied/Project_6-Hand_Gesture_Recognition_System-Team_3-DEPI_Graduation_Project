{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "WGUJu_d-Qyfy",
        "3Pr4ElRaQsDx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing The Libraries**"
      ],
      "metadata": {
        "id": "WGUJu_d-Qyfy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbdy2K9Z9lNR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading The Data for Modelling**"
      ],
      "metadata": {
        "id": "3Pr4ElRaQsDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# 1. Load the processed arrays\n",
        "print(\"Loading data arrays...\")\n",
        "data = np.load('processed_data.npz')\n",
        "X_train, y_train = data['X_train'], data['y_train']\n",
        "X_val, y_val = data['X_val'], data['y_val']\n",
        "X_test, y_test = data['X_test'], data['y_test']\n",
        "\n",
        "# 2. Load metadata\n",
        "print(\"Loading metadata...\")\n",
        "with open('metadata.pkl', 'rb') as f:\n",
        "    metadata = pickle.load(f)\n",
        "\n",
        "label_lookup = metadata['label_lookup']\n",
        "reverse_lookup = metadata['reverse_lookup']\n",
        "num_classes = metadata['num_classes']\n",
        "IMG_SIZE = metadata['IMG_SIZE']\n",
        "\n",
        "# 3. Re-initialize the Data Augmentation (Generators)\n",
        "# Note: We define the generator logic here based on the data loaded\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=25,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.20,\n",
        "    height_shift_range=0.20,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator() # No augmentation for validation\n",
        "\n",
        "# 4. Create the flow from the loaded data\n",
        "train_gen = train_datagen.flow(X_train, y_train, batch_size=64, shuffle=True)\n",
        "val_gen = val_datagen.flow(X_val, y_val, batch_size=64)\n",
        "\n",
        "print(f\"âœ… Setup Complete.\")\n",
        "print(f\"Training Samples: {X_train.shape[0]}\")\n",
        "print(f\"Validation Samples: {X_val.shape[0]}\")\n",
        "print(f\"Number of Classes: {num_classes}\")\n",
        "print(\"You can now proceed to build_cnn()\")"
      ],
      "metadata": {
        "id": "za9Je7JhHzLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "8dfabcf4-af6d-4907-8acd-b1857bd57e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data arrays...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'processed_data.npz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3934880550.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. Load the processed arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data arrays...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_data.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'processed_data.npz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelling**"
      ],
      "metadata": {
        "id": "oS_ObpxnQtIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.30),\n",
        "\n",
        "        layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.40),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.50),\n",
        "\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.0008)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "30RIviO5-Ae6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_cnn((IMG_SIZE, IMG_SIZE, 1), num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "vYsSaBK2-FzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.3, patience=3, min_lr=1e-6)\n",
        "]"
      ],
      "metadata": {
        "id": "okcDm5GM-KaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=15,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "UJ2S0VOB-dJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Performance**"
      ],
      "metadata": {
        "id": "aPp5TKm0Q6mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_df[\"accuracy\"], label=\"Train Acc\")\n",
        "plt.plot(history_df[\"val_accuracy\"], label=\"Val Acc\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_df[\"loss\"], label=\"Train Loss\")\n",
        "plt.plot(history_df[\"val_loss\"], label=\"Val Loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iM3HNwN9-fnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy**"
      ],
      "metadata": {
        "id": "CHDN7IxjQ9k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test Accuracy:\", round(test_acc * 100, 2), \"%\")\n",
        "print(\"Test Loss:\", test_loss)"
      ],
      "metadata": {
        "id": "CFh8iKGe-hj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred_int = np.argmax(y_pred, axis=1)\n",
        "y_true_int = np.argmax(y_test, axis=1)\n",
        "\n",
        "target_names = [reverse_lookup[i] for i in range(num_classes)]\n",
        "\n",
        "print(classification_report(y_true_int, y_pred_int, target_names=target_names))"
      ],
      "metadata": {
        "id": "RR8mH6mn-i83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "PRZdoTLOQ_95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_true_int, y_pred_int)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=target_names,\n",
        "    yticklabels=target_names\n",
        ")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LiSvcRio-kgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a single file\n",
        "# .keras is the modern, zipped format for TensorFlow 2.x\n",
        "model.save('hand_gesture_model.keras')\n",
        "\n",
        "print(\"Model saved successfully as 'hand_gesture_model.keras'\")"
      ],
      "metadata": {
        "id": "lrxQBXSgRgGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}